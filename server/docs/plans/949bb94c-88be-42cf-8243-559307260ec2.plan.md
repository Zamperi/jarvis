PLAN (DRAFT)

RunId: 949bb94c-88be-42cf-8243-559307260ec2
Task: docs/tasks/Ticket-1-Policy.md
Role: coder
Status: APPROVED
TITLE

Enforce Policy Checks for All Write & Exec Tools at Agent Execution Layer

SCOPE
What will change

Add policy enforcement in agentRunner.ts before executing any destructive tool

Apply policy checks to the following tools:

write_file

apply_patch

run_tests

run_build

run_lint

Use checkActionAgainstPolicy(...) before tool execution

Prevent tool execution when policy validation fails

Return structured error results for policy violations

Record policy violations in toolUsage with clear failure reasons

Ensure identical behavior for both OpenAI and Anthropic execution paths

What will NOT change

❌ No changes to fileTools.ts

❌ No changes to execTools.ts

❌ No changes to policyTools.ts

❌ No changes to tool schemas or tool registration

❌ No changes to read-only tools:

read_file

list_files

search_in_files

find_files_by_name

ts_get_outline

DESIGN PRINCIPLE

Policy enforcement is a cross-cutting safety concern and must live in the agent execution layer, not inside individual tools.

Single source of truth:

agentRunner.ts


All policy decisions are made:

with full context (role, mode, projectRoot)

before any destructive action

consistently across providers (OpenAI / Anthropic)

STEP-BY-STEP PLAN
1. Initialize Policy Once per Run

In runAgentInternal (agentRunner.ts):

Build a PolicyConfig using existing helper (buildDefaultPolicy)

Keep this policy object available throughout the execution loop

2. Define Policy-Relevant Actions

Before executing a tool, construct an ActionDescription with appropriate metadata:

write_file

kind: "writeFile"

path: target file path

estimatedChangedLines: derived from content length

apply_patch

kind: "applyPatch"

path: target file path

estimatedChangedLines: derived from patch size

run_tests

kind: "runTests"

run_build

kind: "runBuild"

run_lint

kind: "runLint"

3. Enforce Policy Before Tool Execution

For each destructive tool call:

Call:

checkActionAgainstPolicy(policy, action)


If policy fails:

Do not execute the tool

Return a structured tool error:

{
  ok: false,
  error: "Policy violation: <reason>"
}


Record failure in toolUsage with ok: false and reason

If policy passes:

Proceed with existing tool execution logic unchanged

4. Apply to Both Execution Paths

Implement identical logic in:

OpenAI tool execution loop

Anthropic tool execution loop (tool_use → tool_result)

No behavioral differences between providers.

5. Error Handling & Reporting

Policy violations must:

Be visible to the agent as tool errors

Be logged in toolUsage

Not crash the agent loop

Existing error handling behavior must remain intact

FILES TO TOUCH
1. src/agent/agentRunner.ts (ONLY FILE TO MODIFY)

Reason:

Centralized execution logic

Full context available

Correct place for security and policy enforcement

RISKS & VERIFICATION
Risks

Policy too strict → blocks legitimate actions

Incorrect line estimation → false positives

Missing parity between OpenAI and Anthropic paths

Verification Steps

Attempt to write to .env → must be blocked

Attempt to modify node_modules → must be blocked

Valid write inside project root → must succeed

run_tests allowed when policy permits

Policy violations appear in toolUsage

Read-only tools remain unaffected

Behavior identical for OpenAI and Anthropic

APPROVAL REQUIRED

This plan is intentionally narrow, explicit, and aligned with the existing architecture.

Once approved, implementation can proceed without ambiguity or refactoring risk.